# Configuration file for local vector search
# Copy this file to config.yaml and update with your specific values

# Embedding Model Configuration
huggingface:
  model: "BAAI/bge-large-en-v1.5"  # Replace with your chosen model
  use_gpu: true
  maximum_tokens: 8192 
  batch_size: 32  # Batch size for processing embeddings (higher values use more GPU memory but may be faster)

# ChromaDB Configuration
chromadb:
  path: "./chroma_db"  # Path to store the ChromaDB
  collection_name: "file_embeddings"  # Name of the collection to store embeddings

# File Processing Configuration
file_processing:
  # Whether to show progress bars during processing
  show_progress: true  # Set to false to disable all progress bars
  # File extensions to consider as text-based files
  text_extensions:
    - ".txt"
    - ".md"
    - ".csv"
    - ".ini"
    - ".js"
    - ".py"
    - ".json"
    - ".html"
    - ".css"
    - ".xml"
    - ".yaml"
    - ".yml"
  # Maximum file size in KB to process (to avoid very large files)
  max_file_size_kb: 1024000
  # Directories to exclude from traversal (relative to parent directory)
  exclude_dirs:
    - "venv"
    - ".git"
    - "__pycache__"
    - "node_modules" 
